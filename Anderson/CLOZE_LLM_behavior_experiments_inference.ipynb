{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bF_YN0WlHMt0"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import ast\n",
        "import gc\n",
        "\n",
        "import google.colab.output\n",
        "import google.colab.userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5fu_eqqlzaMU",
        "outputId": "ecff50df-2ca3-429f-ee85-cc301d304796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries successfully installed\n"
          ]
        }
      ],
      "source": [
        "x = !pip install PopulationLM@git+https://github.com/JesseTNRoberts/PopulationLM \\\n",
        "                 minicons@git+https://github.com/JesseTNRoberts/minicons_modded  \\\n",
        "                 accelerate\n",
        "err_lines = [ln for ln in x if 'error' in ln.lower()]\n",
        "\n",
        "google.colab.output.clear()\n",
        "\n",
        "if err_lines:\n",
        "  print(*err_lines, sep='\\n')\n",
        "else:\n",
        "  print('Libraries successfully installed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3C_8ErC5zNsf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForMaskedLM, AutoModelForCausalLM\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from minicons import scorer\n",
        "import PopulationLM as pop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONdOkP9SEfUu",
        "outputId": "a199e2dc-fdf0-429b-b722-a616f05f5a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BuPoTK3IkRCk"
      },
      "outputs": [],
      "source": [
        "def run_experiment(exp_path,\n",
        "                   transformer,\n",
        "                   input_file,\n",
        "                   prompt_col_idx,\n",
        "                   stim_col_idx,\n",
        "                   results_loc=None,\n",
        "                   batch_size=10,\n",
        "                   num_batches=-1,\n",
        "                   committee_size=50,\n",
        "                   debug_early_exit=False,\n",
        "                   cloze=False\n",
        "                  ):\n",
        "  dataset = []\n",
        "  with open(exp_path + '/' + input_file, \"r\") as f:\n",
        "      reader = csv.DictReader(f)\n",
        "      column_names = reader.fieldnames\n",
        "      for row in reader:\n",
        "          dataset.append(list(row.values()))\n",
        "\n",
        "  results = []\n",
        "  control_results = []\n",
        "  conclusion_only = []\n",
        "\n",
        "  column_names += [\"stimulus_prob\"]\n",
        "  with open(results_loc, \"w\", newline='') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow(column_names)\n",
        "\n",
        "  # create a lambda function alias for the method that performs classifications\n",
        "  if cloze:\n",
        "      call_me = lambda p1, q1: transformer.cloze_score(p1, q1)\n",
        "  else:\n",
        "      call_me = lambda p1, q1: transformer.conditional_score(p1, q1, reduction=lambda x: x.mean(0).item())\n",
        "\n",
        "  stimuli_loader = DataLoader(dataset, batch_size = batch_size, num_workers=0)\n",
        "  if num_batches < 0:\n",
        "      num_batches = len(stimuli_loader)\n",
        "  for batch in tqdm(stimuli_loader):\n",
        "      out_dataset = [[] for _ in range(len(batch))]\n",
        "      stim_scores = []\n",
        "\n",
        "      if cloze:\n",
        "        for i in range(len(batch)):\n",
        "          for cell in batch[i]:\n",
        "            for _ in range(4): #TODO: Change this to be not hardcoded number of copies\n",
        "              out_dataset[i].append(cell)\n",
        "      else:\n",
        "        for i in range(len(batch)):\n",
        "            out_dataset[i].extend(batch[i])\n",
        "\n",
        "      results = {'stimulus_prob': [], 'is_correct': [], 'stimulus': []}\n",
        "      p_list = list(batch[prompt_col_idx])\n",
        "      if cloze:\n",
        "          s_list = [ast.literal_eval(s) for s in batch[stim_col_idx]]\n",
        "      else:\n",
        "          s_list = list(batch[stim_col_idx])\n",
        "\n",
        "      population = pop.generate_dropout_population(transformer.model, lambda: call_me(p_list, s_list), committee_size=committee_size)\n",
        "      outs = [item for item in pop.call_function_with_population(transformer.model, population, lambda: call_me(p_list, s_list))]\n",
        "\n",
        "      if cloze:\n",
        "          stim_scores = [[[None for _ in range(committee_size)] for _ in range(len(s_list[b]))] for b in range(len(batch[0]))]\n",
        "          for b in range(len(batch[0])):\n",
        "              for c in range(len(s_list[b])):\n",
        "                  for s in range(committee_size):\n",
        "                      stim_scores[b][c][s] = outs[s][b][c]\n",
        "\n",
        "          answer_choices_per_question = 4\n",
        "          modified_outs = []\n",
        "          tmp_correct = []\n",
        "          for question in range(len(stim_scores)):\n",
        "            for choice in range(answer_choices_per_question):\n",
        "                modified_outs.append(stim_scores[question][choice])\n",
        "\n",
        "            results['is_correct'].extend([int(batch[2][question] == s) for s in ast.literal_eval(batch[1][question])])\n",
        "\n",
        "            results['stimulus'].extend(s_list[question])\n",
        "\n",
        "          results['stimulus_prob'].extend(modified_outs)\n",
        "\n",
        "      else:\n",
        "          transposed_outs = [[row[i] for row in outs] for i in range(len(outs[0]))]\n",
        "\n",
        "          stim_scores = [score for score in transposed_outs]\n",
        "\n",
        "          results['stimulus_prob'].extend(stim_scores)\n",
        "\n",
        "      out_dataset.append(results['stimulus_prob'])\n",
        "      if cloze:\n",
        "        out_dataset[1] = results['stimulus']\n",
        "        out_dataset[2] = results['is_correct']\n",
        "      with open(results_loc, \"a\", newline='') as f:\n",
        "          writer = csv.writer(f)\n",
        "          writer.writerows(list(zip(*out_dataset)))\n",
        "\n",
        "      if debug_early_exit:\n",
        "          break\n",
        "\n",
        "  del population\n",
        "\n",
        "  print('\\nResults saved to: ', results_loc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BWipNRUxbRrh"
      },
      "outputs": [],
      "source": [
        "def run_all_experiments(model,\n",
        "                        lm_type,\n",
        "                        input_file,\n",
        "                        prompt_col_idx,\n",
        "                        stim_col_idx,\n",
        "                        batch_size=10,\n",
        "                        committee_size=50,\n",
        "                        run_base_model=False,\n",
        "                        drive_loc=None,\n",
        "                        save_name=None,\n",
        "                        token=None,\n",
        "                        debug_early_exit=False):\n",
        "    # Detect file_not_found errors before loading model to save time and RAM\n",
        "    for exp in experiments:\n",
        "          for ds, cloze in datasets:\n",
        "            exp_path = drive_loc  + '/' + ds + '/' + exp + '/' + input_file\n",
        "            if not os.path.isfile(exp_path):\n",
        "                raise FileNotFoundError(exp_path)\n",
        "\n",
        "    base_model_name = model\n",
        "    device='cuda'\n",
        "\n",
        "    # Load the model\n",
        "    if lm_type == \"masked\" or lm_type == \"mlm\":\n",
        "        try:\n",
        "          transformer = scorer.MaskedLMScorer(base_model_name,\n",
        "                                              device=device,\n",
        "                                              local_files_only=False,\n",
        "                                              low_cpu_mem_usage=True,\n",
        "                                              torch_dtype=torch.float16,\n",
        "                                              device_map=\"auto\",\n",
        "                                              token=token)\n",
        "        except:\n",
        "          transformer = scorer.MaskedLMScorer(base_model_name, device=device, token=token)\n",
        "    elif lm_type == \"incremental\" or lm_type == \"causal\":\n",
        "        try:\n",
        "          transformer = scorer.IncrementalLMScorer(base_model_name,\n",
        "                                                   device=device,\n",
        "                                                   local_files_only=False,\n",
        "                                                   low_cpu_mem_usage=True,\n",
        "                                                   torch_dtype=torch.float16,\n",
        "                                                   device_map=\"auto\",\n",
        "                                                   token=token)\n",
        "        except:\n",
        "          transformer = scorer.IncrementalLMScorer(base_model_name, device=device, token=token)\n",
        "\n",
        "    #Overwrite local model with base model (handles local loading limitation in minicons)\n",
        "    if save_name is not None:\n",
        "        model_name = save_name\n",
        "    else:\n",
        "        model_name = base_model_name\n",
        "\n",
        "    if \"/\" in model_name:\n",
        "      model_name = model_name.replace(\"/\", \"_\")\n",
        "\n",
        "    try:\n",
        "        if run_base_model:\n",
        "            for exp in experiments:\n",
        "              for ds, cloze in datasets:\n",
        "                print(f'Running experiment (base): {exp}')\n",
        "                exp_path = drive_loc  + '/' + ds + '/' + exp\n",
        "                run_experiment(exp_path,\n",
        "                              transformer,\n",
        "                              input_file,\n",
        "                              prompt_col_idx,\n",
        "                              stim_col_idx,\n",
        "                              results_loc=exp_path + f\"/{model_name}_base_results.csv\",\n",
        "                              batch_size=batch_size,\n",
        "                              committee_size=1,\n",
        "                              debug_early_exit=debug_early_exit,\n",
        "                              cloze=cloze\n",
        "                              )\n",
        "                gc.collect()\n",
        "\n",
        "\n",
        "        if committee_size > 0:\n",
        "            pop.DropoutUtils.add_new_dropout_layers(transformer.model)\n",
        "            pop.DropoutUtils.convert_dropouts(transformer.model)\n",
        "            pop.DropoutUtils.activate_mc_dropout(transformer.model, activate=True, random=0.1)\n",
        "\n",
        "            for exp in experiments:\n",
        "              for ds, cloze in datasets:\n",
        "                print(f'Running experiment (population): {exp}')\n",
        "                exp_path = drive_loc  + '/' + ds + '/' + exp\n",
        "                run_experiment(exp_path,\n",
        "                              transformer,\n",
        "                              input_file,\n",
        "                              prompt_col_idx,\n",
        "                              stim_col_idx,\n",
        "                              results_loc=exp_path + f\"/{model_name}_pop{committee_size}_results.csv\",\n",
        "                              batch_size=batch_size,\n",
        "                              committee_size=committee_size,\n",
        "                              debug_early_exit=debug_early_exit,\n",
        "                              cloze=cloze\n",
        "                              )\n",
        "                gc.collect()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "    finally:\n",
        "        del transformer\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ISSw04F7YwD4"
      },
      "outputs": [],
      "source": [
        "#Replace with experiment base directory\n",
        "drive_loc='/content/drive/MyDrive/evaluating_fan_effects_in_large_language_models/Experiments/Anderson/data/Random'\n",
        "\n",
        "#Name of the file where prompts are stored. Assumed to be a csv file with '|' delimiters\n",
        "input_file = 'prompts.csv'\n",
        "\n",
        "#Each experiment name should be the name a directory that descends from drive_loc\n",
        "experiments = [\n",
        "                # 'And_True',\n",
        "                'Random'\n",
        "              ]\n",
        "\n",
        "datasets = [\n",
        "    ('run_0', False),\n",
        "    ('run_1', False),\n",
        "    ('run_2', False),\n",
        "]\n",
        "\n",
        "# batch_size should be carefully chosen based on planned analysis. Because each\n",
        "# batch uses a separately-generated population (limitation enforced by pytorch\n",
        "# implementation details), any trials that are to be compared should be, where\n",
        "# possible, included in the same batch. For example, MMLU uses multiple choice\n",
        "# questions with 4 options. Because comparisons are to be made across answers\n",
        "# to the same question, we should avoid splitting a question across multiple\n",
        "# batches. To avoid this, batch_size is chosen to be a multiple of 4.\n",
        "batch_size = 50\n",
        "\n",
        "# committee_size is arbitrarily set at 50. No need to change until future\n",
        "# investigation into appropriate population sizes. If committee_size = 0,\n",
        "# then no population will be used (useful if only running the base model).\n",
        "# Output file name will be of the form\n",
        "#   {drive_loc}/{exp_name}/{model_name}_pop{committee_size}_results.csv\n",
        "committee_size = 50\n",
        "\n",
        "# if run_base_model is True, then the base model (no dropout) is tested in addition\n",
        "# to the model. Output file name will be of the form\n",
        "#   {drive_loc}/{exp_name}/{model_name}_base_results.csv\n",
        "run_base_model = False\n",
        "\n",
        "#Each model should be defined using the format (model_name, model_type, save_name)\n",
        "#model_name should be the name used to load the model using the transformers library (i.e. either a file location or the huggingface name)\n",
        "#model_type should be one of 'masked' or 'incremental', depending on the model structure\n",
        "#save_name is where the results will be the name used when creating the results file. Results file name will be <drive_loc>/<experiment_name>/results_<save_name>.csv\n",
        "models = [\n",
        "    # ('openai-community/gpt2',               'incremental', 'GPT2',            ),\n",
        "    # ('meta-llama/Llama-2-7b-hf',            'incremental', 'LLaMa2-7B',       ),\n",
        "    # ('meta-llama/Meta-Llama-3-8B',          'incremental', 'LLaMa3-8B',       ),\n",
        "    ('mistralai/Mistral-7B-v0.1',           'incremental', 'Mistral-7B',      ),\n",
        "    ('upstage/SOLAR-10.7B-v1.0',            'incremental', 'SOLAR-10-7B',     ),\n",
        "]\n",
        "#phi-2 resumt counterfactual at hs_macroecon\n",
        "#control broke for gpt-2 and gemma and phi-2\n",
        "\n",
        "#These are the zero-based index of the prompt and the stimulus in the input csv file\n",
        "prompt_col_index = 0\n",
        "stim_col_index = 5\n",
        "\n",
        "#Forces above cells to only run one iteration for testing purposes. Set to False when running full experiment\n",
        "debug_EE = False\n",
        "\n",
        "# Handles my_token undefined. If you want to use a huggingface token (necessary\n",
        "# for LLaMa models among others), run the cell above this one\n",
        "try:\n",
        "    my_token = google.colab.userdata.get('hf_token')\n",
        "except userdata.SecretNotFoundError:\n",
        "    print('/************************************ALERT************************************\\\\')\n",
        "    print('|    Token not found in secrets. Either add huggingface token to secrets or   |')\n",
        "    print('|    check that the secret name matches the argumnet to userdata.get. If      |')\n",
        "    print('|    the models being used do not need a token, this alert can be ignored.    |')\n",
        "    print('\\\\************************************ALERT************************************/')\n",
        "\n",
        "    my_token = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aQqJW5dJYgV"
      },
      "outputs": [],
      "source": [
        "# format: (model_name, model_type, save_name)\n",
        "for mn, mt, sn in models:\n",
        "    run_all_experiments(mn, mt,\n",
        "                        input_file,\n",
        "                        prompt_col_index,\n",
        "                        stim_col_index,\n",
        "                        batch_size=batch_size,\n",
        "                        committee_size=committee_size,\n",
        "                        run_base_model=run_base_model,\n",
        "                        save_name=sn,\n",
        "                        drive_loc=drive_loc,\n",
        "                        token=my_token,\n",
        "                        debug_early_exit=debug_EE\n",
        "                        )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}